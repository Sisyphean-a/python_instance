## requests库

### 基础函数

| 函数               | 作用                                           |
| ------------------ | ---------------------------------------------- |
| requests.requset() | 构造一个请求，支撑以下各方法的基础方法         |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的get        |
| requests.head()    | 获取HTML网页头信息的方法，对应于HTTP的head     |
| requests.post()    | 向HTML网页提交POST请求的方法，对应于HTTP的POST |
| requests.put()     | 向HTML网页提交pot请求的方法，对应于HTTP的pot   |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的patch  |
| requests.deleta()  | 向HTML页面提交删除请求，对应于HTTP的deleta     |

post:请求向url位置的资源后附加新的数据。即：部分替换

put:请求向url的位置存储一个资源，覆盖原url位置的资源。即：全部替换

### response对象的五个属性

r=requests.get(url， params=None，**kwarge)

| 属性                | 含义                                                         |
| ------------------- | ------------------------------------------------------------ |
| r.status_code       | 返回状态码，如果是200，则访问成功，404表示失败               |
| r.text              | HTTP响应内容的字符串形式，即，url对应的页面内容              |
| r.encoding          | 从HTTP header中猜测的响应内容编码方式。如果 header中不存在 charset，则认为编码为ISO-8859-1 |
| r.apparent_encoding | 从内容中分析出的响应内容编码方式（备用编码方式）             |
| r.content           | HTTP响应内容的二进制形式                                     |

### requests基础流程

1. 由r.status_code先判断状态码，如果是200，进行步骤2，如果不是200，进行步骤3
2. 如果响应码是200，则可以使用r.text  r.encoding  r.apparent_encoding  r.content
3. 如果响应码是400或其他原因出错，将产生异常

### requests库异常

| 异常函数                  | 异常原因                                |
| ------------------------- | --------------------------------------- |
| requests.ConnectionError  | 网络连接异常，如DNS查询失败，拒绝连接等 |
| requests.HTTPError        | http错误异常                            |
| requests.URLRequired      | url缺失异常                             |
| requests.TooManyRedirects | 超过最大重定向次数，产生重定向异常      |
| requests.ConnectTimeout   | 连接远程服务器超时异常                  |
| requests.Timeout          | 请求url超时，产生超时异常               |

### 爬取网页通用框架

```python
import requests
def getHTMLText(url):
	try:
		r=requests.get(url，timeout=30)   #我使用的是get，故申请的内容为全部内容
		r.raise_for_status()
		r.encoding=r.apparent_encoding
		return r.text			#我使用的是text，故展示的是全部内容。
	except:				#如果我使用的是head,调用的资源是全部内容但仅会展示头文件
		return "产生异常“
if  __name__=="__main__"
		url="http://www.baidu.com"
		print(getHTTPText(url))
```

### requests的七种方法

```python
requests.request(method,url,**kwargs)
		method:就是get,head,post,put等参数
		kwargs:控制访问的参数，均为可选项:
		params:字典或字节序列，作为参数增加到url中
		*data:字典，字节序列或文件对象，作为request的内容
		*json:json格式的数据，作为request的内容
		*headers:字典，HTTP定制头
		cookies:字典或cookiejar，request中的cookie
		auth：元组，支持HTTP认证功能
		files:字典类型，传输文件
		timeout：设定超时时间，秒为单位
		proxies:字典类型，设定访问代理服务器，可以增加登录认证
		
requests.get(url,params=none,**kwargs)  #重要

requests.head(url,**kwargs)	#重要
		
requests.post(url,data=None,json=None,**kwargs)

requests.put(url,data=None,**kwargs)

requests.patch(url,data=None,**kwargs)

requests.delete(url,**kwargs)
```

### 网络爬虫的限制

- 来源审查：使用user-agent进行限制
		检查来访HTTP协议头的user-agent域，只响应浏览器或友好爬虫的访问
- 发布公告：robots协议
		告知爬虫此网站规则

#### 如何应对来源审查

修改头文件，即headers方法。定义一个user-agent，然后赋予url	

```python
kv={'user-agent':'Mozilla/5.0'}
url=""
r=requests.get(url,headers=kv)
```

#### 全代码

```python
import requests
def getHTTP(url)
	try:
		kv={'user-agent':'Mozilla/5.0'}
		r=requests.get(url,headers=kv)
		r.raise_for_status()
		r.encoding=r.apparent_encoding
		return r.text
	except:
		return "出现错误！"
		
if  __name__="__main__"
	url="www.baidu.com"
	print(getHTTP(url))
```

### 图片的爬取

网络上图片的格式为网址+图片格式，例如：http://*****.com/*****.jpg

```python
import requests
import os

url="http://******.jpg"
root="D://pics//"
path=root+url.split('/')[-1]

try:
	if not os.path.exists(root):   
		os.mkdir(root)
	if not os.path.exists(path):
		r=requests.get(url)
		with open(path,'wb') as f:
			f.write(r.content)
			f.close()
			print("文件保存成功")
	else:
		print("文件已存在")
except:
	print("爬取失败")
    
# os.path.exists(path),判断路径是否存在
# 如果路径存在，则返回一个True,如果不存在，就返回一个false
# os.mkdir(path),创建一个路径
# if not语句 如果否或者空，则执行内部语句
```



## Beautiful Soup库

Beautuful Soup是解析，遍历，维护“标签树”的功能库
### 库的调用

```python
# 调用beautifulsoup库：
from bs4 import BeautifulSoup

# 煲汤
soup=BeautifulSoup(r.text,"html.parser")
```

注意的是，这个煲汤需要一个解释器，类似于需要一种“火”，我们这里用的火就是html.parser
没有火就没有办法煲汤，然后我们煲汤的主料是一个HTML文档，即我们爬取到的r.text

### Beautuful Soup类的基本元素

注意，这些都是煲好汤之后对汤的操作，也许可以说是汤的调料

1. Tag:标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾
				使用方式：直接用煲好的汤加上.标签即可，例：soup.a
2. Name:标签的名字，\<p>...\</p>的名字是‘p’，格式：\<tag>.name 
				即：soup.a.name,也可以查看a标签的父标签的名字：soup.a.parent.name
3. Attributes:标签的属性，字典形式组织，格式：\<tag>.attrs
				即：soup.a.attrs,输出的是键值对字典模式，所以可以使用字典的方式进行提取
				例如：soup.a.attrs['class']
4. NavigableString:标签内非属性字符串，<>...</>中字符串，格式：\<tag>.string
				即：soup.a.string
5. Comment:标签内字符串的注释部分，一种特殊的Comment类型,方式同上

### 基于bs4库的HTML遍历方法

#### 下行遍历
1. .contents:子节点的列表，将\<tag>所有的儿子节点存入列表
	​		返回的是列表，可以用列表的方式进行检索.
	​		例:len(soup.body.contents),然后：soup.body.contents[1]
2. .children:子节点的迭代类型，与.contents类似，用于循环遍历儿子节点
	​		遍历：for child in soup.body.children:
	​				print(child)
3. .descendants:子孙节点的迭代类型，包含所有子孙节点，用于遍历循环
	​		遍历:同上
	​注意：contents返回的是列表，而后面两个返回的是迭代类型，只能用于遍历

#### 上行遍历
1. .parent:节点的父亲标签
2. .parents:节点的先辈标签的迭代类型，用于循环遍历先辈节点

#### 平行遍历
​	注意：所有的平行遍历发生在同一个父节点下

1. .next_sibling:返回按照HTML文本顺序的下一个平行节点标签
2. .previous_sibling:返回按照HTML文本顺序的下一个平行节点标签
3. .next_siblings:迭代类型，返回按照HTML文本顺序的后续所有平行节点标签
	​		遍历:for sibling in soup.a.next_siblings:
	​				print(sibling)
4. .previous_siblings:迭代类型，返回按照HTML文本顺序的前续所有平行节点标签
	​		遍历:同上

> bs4库的prettify()方法：
> prettify方法能为HTML文本增加换行符，通过print(soup.prettify())输出的文本很美观
> 同时，prettify方法也能为单个标签进行美化，例如：print(soup.a.prettify())
> 注意：bs4库把所有读入的HTML文件和字符串的编码格式都转换为了utf-8的格式

### 信息标记

**信息标记的作用**

1. 标记后的信息可形成信息组织结构，增加了信息维度
2. 标记后的信息可用于通信，存储或展示
3. 标记的结构与信息一样具有重要价值
4. 标记后的信息更利于程序理解和运用

#### XML

​	类似于HTML格式，可以说是HTML的一种扩展

- 如果标签内有信息，那么就用一对标签表示：\<name>...\</name>
- 如果标签内没有信息，那么就用一个标签直接表示：\<name/>
- 关于注释，类似于HTML，即：\<!--   -->
- 扩展性好，但繁琐，多用于Internet的信息交互与传递

#### JSON

​	javascript语言中面向对象的一种表达形式，有类型的键值对的表达形式

- 例如：“name”：“北京理工大学”
- 注意，JSON中所有的字符串都要带上双引号，如果值是数字，不用加双引号
- 当一个值中含有多个信息时，采用方括号加逗号的形式：“name”：[“北大”,"清华"]
- 键值对嵌套时用{，}的形式："name":{"newname":"北京理工大学"
										  "oldname":"延安自然科学院"}
- 适合程序处理，较xml简洁，多用于移动应用云端和节点的信息通信，没有注释

#### YAML

​	无类型的键值对，即不用加双引号。类似于python    *很实用

- 所诉关系通过缩进的形式进行表达，用减号-表示并列关系
- 例：name:
				-北京理工大学
				-延安自然科学院
- 用|表示大段文字，用#表示注释
- 文本信息比例最高，可读性好，各类系统的配置文件，有注释，易读

### 信息提取的一般方法

1. 完整解析信息的标记形式，再提取关键信息。
	- xml,json,yaml
	- 需要标记解析器，例如：bs4库的标签树遍历
	- 优点：信息解析准确
	- 缺点：提取过程繁琐，速度慢
2. 无视标记形式，直接搜索关键信息。
	- 搜索  对信息的文本查找函数即可
	- 优点：提取过程简洁，速度较快。
	- 缺点：提取结果准确性与信息内容相关
3. 融合方法：结合形式解析与搜索方法，提取关键信息。
	- xml,json,yaml 搜索
	- 需要标记解析器及文本查找函数



